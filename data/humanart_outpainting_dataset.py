"""
HumanArt Dataset for Single-shot Center-to-Full Outpainting
ì¤‘ì•™ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ ì „ì²´ ìº”ë²„ìŠ¤ë¡œ í™•ì¥í•˜ëŠ” outpainting í•™ìŠµìš© ë°ì´í„°ì…‹
"""
import os
import json
import torch
import numpy as np
from PIL import Image, ImageOps
from torch.utils.data import Dataset
from torchvision import transforms
from diffusers import DDPMScheduler
import random

class HumanArtOutpaintingDataset(Dataset):
    def __init__(
        self,
        dataset_root="/app/datasets/humanart",
        mapping_file="mapping_file_training.json",
        target_size=512,      # ìµœì¢… ì´ë¯¸ì§€ í¬ê¸°
        center_size=256,      # ì¤‘ì•™ ì˜ì—­ í¬ê¸° (ë…¼ë¬¸ì˜ r=0.5: 50%Ã—50% = 25% ë©´ì )
        noise_scheduler=None,
        transform=None,
        use_2d_artwork_only=True,  # 2D artworkë§Œ ì‚¬ìš©
        excluded_categories=['cartoon', 'digital_art', 'kids_drawing', 'shadow_play', 'sketch']  # ì œì™¸í•  ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’)
    ):
        self.dataset_root = dataset_root
        self.target_size = target_size
        self.center_size = center_size
        self.use_2d_artwork_only = use_2d_artwork_only
        self.excluded_categories = excluded_categories
        
        # DDPM ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ (SD 1.5ì™€ ë™ì¼í•œ ì„¤ì •)
        if noise_scheduler is None:
            self.noise_scheduler = DDPMScheduler(
                num_train_timesteps=1000,
                beta_start=0.00085,
                beta_end=0.012,
                beta_schedule="scaled_linear",
                clip_sample=False,
                steps_offset=1,
                prediction_type="epsilon"
            )
        else:
            self.noise_scheduler = noise_scheduler
            
        # ë°ì´í„°ì…‹ ë¡œë“œ
        mapping_path = os.path.join(dataset_root, mapping_file)
        with open(mapping_path, 'r') as f:
            self.data_mapping = json.load(f)
        
        self.data_keys = list(self.data_mapping.keys())
        original_count = len(self.data_keys)
        print(f"âœ“ HumanArt ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {original_count}ê°œ ì´ë¯¸ì§€")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§
        if self.use_2d_artwork_only or self.excluded_categories:
            filtered_keys = []
            category_counts = {}  # ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜ ì¶”ì 
            excluded_count = 0
            
            for key in self.data_keys:
                img_path = self.data_mapping[key]['img_path']
                
                # 2D artworkë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
                if self.use_2d_artwork_only and not img_path.startswith('images/2D_virtual_human/'):
                    excluded_count += 1
                    continue
                    
                # ì œì™¸ ì¹´í…Œê³ ë¦¬ ì²´í¬
                should_exclude = False
                if self.excluded_categories and img_path.startswith('images/2D_virtual_human/'):
                    # img_pathì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ: images/2D_virtual_human/cartoon/xxx.jpg -> cartoon
                    path_parts = img_path.split('/')
                    if len(path_parts) >= 3:
                        category = path_parts[2]
                        if category in self.excluded_categories:
                            should_exclude = True
                            excluded_count += 1
                        else:
                            # í¬í•¨ëœ ì¹´í…Œê³ ë¦¬ ì¹´ìš´íŠ¸
                            category_counts[category] = category_counts.get(category, 0) + 1
                
                if not should_exclude:
                    filtered_keys.append(key)
            
            self.data_keys = filtered_keys
            print(f"âœ“ ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì™„ë£Œ: {original_count}ê°œ â†’ {len(self.data_keys)}ê°œ ì´ë¯¸ì§€ ({excluded_count}ê°œ ì œì™¸)")
            
            if category_counts:
                print("âœ“ ì‚¬ìš©ëœ 2D virtual human ì¹´í…Œê³ ë¦¬:")
                for category, count in sorted(category_counts.items()):
                    print(f"  - {category}: {count:,}ê°œ")
        
        # ê¸°ë³¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        if transform is None:
            self.transform = transforms.Compose([
                transforms.Resize((target_size, target_size), interpolation=transforms.InterpolationMode.BILINEAR),
                transforms.ToTensor(),
                transforms.Normalize([0.5], [0.5])  # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
            ])
        else:
            self.transform = transform
            
        # ì¤‘ì•™ í¬ë¡­ì„ ìœ„í•œ ê³„ì‚°
        self.crop_offset = (target_size - center_size) // 2
        
    def __len__(self):
        return len(self.data_keys)
    
    def load_image(self, img_path):
        """ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        full_path = os.path.join(self.dataset_root, img_path)
        
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(full_path).convert('RGB')
            
            # ê°•ì œ ë¦¬ì‚¬ì´ì§• (aspect ratio ë¬´ì‹œí•˜ê³  ì •ì‚¬ê°í˜•ìœ¼ë¡œ)
            max_size = max(image.size)
            image = image.resize((max_size, max_size), Image.LANCZOS)
            
            return image
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ {img_path}: {e}")
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            return Image.new('RGB', (512, 512), color=(128, 128, 128))
    
    def create_center_crop_setup(self, original_image):
        """
        ì¤‘ì•™ í¬ë¡­ ì„¤ì • ìƒì„±
        - ì›ë³¸ ì´ë¯¸ì§€ë¥¼ target_sizeë¡œ ë¦¬ì‚¬ì´ì¦ˆ (Ground Truth)
        - ì¤‘ì•™ center_size ì˜ì—­ë§Œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ë§ˆìŠ¤í‚¹
        """
        # 1. ì›ë³¸ ì´ë¯¸ì§€ë¥¼ target_sizeë¡œ ë¦¬ì‚¬ì´ì¦ˆ (Ground Truth)
        gt_image = original_image.resize((self.target_size, self.target_size), Image.LANCZOS)
        gt_tensor = self.transform(gt_image)  # [3, H, W], [-1, 1] ë²”ìœ„
        
        # 2. ì¤‘ì•™ ì˜ì—­ ì¶”ì¶œ
        center_crop = gt_image.crop((
            self.crop_offset, 
            self.crop_offset,
            self.crop_offset + self.center_size,
            self.crop_offset + self.center_size
        ))
        
        # 3. ì…ë ¥ ì´ë¯¸ì§€ ìƒì„± (ì¤‘ì•™ì— center_crop ë°°ì¹˜, ë‚˜ë¨¸ì§€ëŠ” íšŒìƒ‰)
        input_image = Image.new('RGB', (self.target_size, self.target_size), color=(128, 128, 128))
        input_image.paste(center_crop, (self.crop_offset, self.crop_offset))
        input_tensor = self.transform(input_image)  # [3, H, W]
        
        # 4. ë§ˆìŠ¤í¬ ìƒì„± (ì¤‘ì•™ ì˜ì—­ì€ 0, ë‚˜ë¨¸ì§€ëŠ” 1)
        mask = torch.ones(1, self.target_size, self.target_size)  # [1, H, W]
        mask[:, self.crop_offset:self.crop_offset+self.center_size, 
             self.crop_offset:self.crop_offset+self.center_size] = 0
        
        return gt_tensor, input_tensor, mask
    
    def add_diffusion_noise(self, clean_image):
        """Diffusion ë…¸ì´ì¦ˆ ì¶”ê°€"""
        # ëœë¤ íƒ€ì„ìŠ¤í… ì„ íƒ
        timesteps = torch.randint(0, self.noise_scheduler.config.num_train_timesteps, (1,))
        
        # ë…¸ì´ì¦ˆ ìƒì„±
        noise = torch.randn_like(clean_image)
        
        # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì´ë¯¸ì§€ ìƒì„±
        noisy_image = self.noise_scheduler.add_noise(clean_image, noise, timesteps)
        
        return noisy_image, noise, timesteps
    
    def __getitem__(self, idx):
        """ë°ì´í„°ì…‹ ì•„ì´í…œ ë°˜í™˜"""
        # ë°ì´í„° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        data_key = self.data_keys[idx]
        data_info = self.data_mapping[data_key]
        
        img_path = data_info['img_path']
        prompt = data_info['prompt']
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        original_image = self.load_image(img_path)
        
        # ì¤‘ì•™ í¬ë¡­ ì„¤ì • ìƒì„±
        gt_image, input_image, mask = self.create_center_crop_setup(original_image)
        
        # Diffusion ë…¸ì´ì¦ˆ ì¶”ê°€ (Ground Truthì—)
        noisy_gt, noise_target, timesteps = self.add_diffusion_noise(gt_image)
        
        # SD 1.5 Inpainting í˜•ì‹ìœ¼ë¡œ ì…ë ¥ êµ¬ì„± (9ì±„ë„)
        # [noisy_latent:4 + mask:1 + masked_image:4] = 9ì±„ë„
        # ì—¬ê¸°ì„œëŠ” RGB ì´ë¯¸ì§€ì´ë¯€ë¡œ [noisy_gt:3 + mask:1 + input_image:3] = 7ì±„ë„
        # ì‹¤ì œ í•™ìŠµì—ì„œëŠ” VAE latent spaceì—ì„œ 4ì±„ë„ë¡œ ë³€í™˜ë¨
        
        model_input = torch.cat([
            noisy_gt,     # [3, H, W] - ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ GT ì´ë¯¸ì§€
            mask,         # [1, H, W] - ë§ˆìŠ¤í¬ (outpainting ì˜ì—­ = 1)
            input_image   # [3, H, W] - ì¤‘ì•™ ì´ë¯¸ì§€ê°€ ìˆëŠ” ì…ë ¥
        ], dim=0)  # [7, H, W]
        
        return {
            'model_input': model_input,     # [7, H, W] - ëª¨ë¸ ì…ë ¥
            'noise_target': noise_target,   # [3, H, W] - ë…¸ì´ì¦ˆ ì˜ˆì¸¡ íƒ€ê²Ÿ
            'timesteps': timesteps,         # [1] - íƒ€ì„ìŠ¤í…
            'prompt': prompt,               # str - í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
            'gt_image': gt_image,          # [3, H, W] - Ground Truth ì´ë¯¸ì§€
            'input_image': input_image,     # [3, H, W] - ì…ë ¥ ì´ë¯¸ì§€ (ì¤‘ì•™ë§Œ)
            'mask': mask,                  # [1, H, W] - ë§ˆìŠ¤í¬
            'data_key': data_key           # str - ë°ì´í„° í‚¤
        }

def collate_fn(batch):
    """ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬"""
    model_inputs = torch.stack([item['model_input'] for item in batch])
    noise_targets = torch.stack([item['noise_target'] for item in batch])
    timesteps = torch.stack([item['timesteps'] for item in batch]).squeeze()
    prompts = [item['prompt'] for item in batch]
    gt_images = torch.stack([item['gt_image'] for item in batch])
    input_images = torch.stack([item['input_image'] for item in batch])
    masks = torch.stack([item['mask'] for item in batch])
    data_keys = [item['data_key'] for item in batch]
    
    return {
        'model_inputs': model_inputs,      # [B, 7, H, W]
        'noise_targets': noise_targets,    # [B, 3, H, W]
        'timesteps': timesteps,            # [B]
        'prompts': prompts,                # List[str]
        'gt_images': gt_images,           # [B, 3, H, W]
        'input_images': input_images,      # [B, 3, H, W]
        'masks': masks,                   # [B, 1, H, W]
        'data_keys': data_keys            # List[str]
    }

def create_dataloader(
    dataset_root="/app/datasets/humanart",
    batch_size=4,
    num_workers=4,
    shuffle=True,
    target_size=512,
    center_size=256,  # ë…¼ë¬¸ì˜ r=0.5 ì„¤ì •
    use_2d_artwork_only=True,  # 2D artworkë§Œ ì‚¬ìš©
    excluded_categories=['cartoon', 'digital_art', 'kids_drawing', 'shadow_play', 'sketch']  # ì œì™¸í•  ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’)
):
    """ë°ì´í„°ë¡œë” ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    
    dataset = HumanArtOutpaintingDataset(
        dataset_root=dataset_root,
        target_size=target_size,
        center_size=center_size,
        use_2d_artwork_only=use_2d_artwork_only,
        excluded_categories=excluded_categories
    )
    
    from torch.utils.data import DataLoader
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )
    
    return dataloader, dataset

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª HumanArt Outpainting Dataset í…ŒìŠ¤íŠ¸")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = HumanArtOutpaintingDataset(
        target_size=512,
        center_size=320
    )
    
    # ìƒ˜í”Œ í™•ì¸
    sample = dataset[0]
    print(f"âœ“ ëª¨ë¸ ì…ë ¥ shape: {sample['model_input'].shape}")
    print(f"âœ“ ë…¸ì´ì¦ˆ íƒ€ê²Ÿ shape: {sample['noise_target'].shape}")
    print(f"âœ“ íƒ€ì„ìŠ¤í…: {sample['timesteps']}")
    print(f"âœ“ í”„ë¡¬í”„íŠ¸: {sample['prompt'][:50]}...")
    print(f"âœ“ GT ì´ë¯¸ì§€ shape: {sample['gt_image'].shape}")
    print(f"âœ“ ì…ë ¥ ì´ë¯¸ì§€ shape: {sample['input_image'].shape}")
    print(f"âœ“ ë§ˆìŠ¤í¬ shape: {sample['mask'].shape}")
    
    # ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸
    dataloader, _ = create_dataloader(batch_size=2, num_workers=0)
    batch = next(iter(dataloader))
    
    print(f"\nğŸ“¦ ë°°ì¹˜ í…ŒìŠ¤íŠ¸:")
    print(f"âœ“ ë°°ì¹˜ ëª¨ë¸ ì…ë ¥ shape: {batch['model_inputs'].shape}")
    print(f"âœ“ ë°°ì¹˜ ë…¸ì´ì¦ˆ íƒ€ê²Ÿ shape: {batch['noise_targets'].shape}")
    print(f"âœ“ ë°°ì¹˜ íƒ€ì„ìŠ¤í… shape: {batch['timesteps'].shape}")
    print(f"âœ“ ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ê°œìˆ˜: {len(batch['prompts'])}")
    
    print("ğŸ‰ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")